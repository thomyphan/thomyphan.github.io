@article{phanJAIR25,
    author      = {Thomy Phan and Timy Phan and Sven Koenig},
    title       = {Generative Curricula for Multi-Agent Path Finding via Unsupervised and Reinforcement Learning},
    url         = {https://thomyphan.github.io/publication/2025-04-01-jair-phan},
    abstract    = {Multi-Agent Path Finding (MAPF) is the challenging problem of finding collision-free paths for multiple agents, which has a wide range of applications, such as automated warehouses, smart manufacturing, and traffic management. Recently, machine learning-based approaches have become popular in addressing MAPF problems in a decentralized and potentially generalizing way. Most learning-based MAPF approaches use reinforcement and imitation learning to train agent policies for decentralized execution under partial observability. However, current state-of-the-art approaches suffer from a prevalent bias to micro-aspects of particular MAPF problems, such as congestions in corridors and potential delays caused by single agents, leading to tight specializations through extensive engineering via oversized models, reward shaping, path finding algorithms, and communication. These specializations are generally detrimental to the sample efficiency, i.e., the learning progress given a certain amount of experience, and generalization to previously unseen scenarios. In contrast, curriculum learning offers an elegant and much simpler way of training agent policies in a step-by-step manner to master all aspects implicitly without extensive engineering. In this paper, we propose a generative curriculum approach to learning-based MAPF using Variational Autoencoder Utilized Learning of Terrains (VAULT). We introduce a two-stage framework to (I) train the VAULT via unsupervised learning to obtain a latent space representation of maps and (II) use the VAULT to generate curricula in order to improve sample efficiency and generalization of learning-based MAPF methods. For the second stage, we propose a bi-level curriculum scheme by combining our VAULT curriculum with a low-level curriculum method to improve sample efficiency further. Our framework is designed in a modular and general way, where each proposed component serves its purpose in a black-box manner without considering specific micro-aspects of the underlying problem. We empirically evaluate our approach in maps of the public MAPF benchmark set as well as novel artificial maps generated with the VAULT. Our results demonstrate the effectiveness of the VAULT as a map generator and our VAULT curriculum in improving sample efficiency and generalization of learning-based MAPF methods compared to alternative approaches. We also demonstrate how data pruning can further reduce the dependence on available maps without affecting the generalization potential of our approach.},
    journal     = {Journal of Artificial Intelligence Research (JAIR)},
    year        = {2025},
    volume      = {82},
    eprint      = {https://thomyphan.github.io/files/2025-jair.pdf},
    doi         = {https://doi.org/10.1613/jair.1.17403},
    pages       = {2471--2534}
}

@article{phan1AAAI25,
    author      = {Thomy Phan* and Benran Zhang* and Shao-Hung Chan and Sven Koenig},
    title       = {Anytime Multi-Agent Path Finding with an Adaptive Delay-Based Heuristic},
    url         = {https://thomyphan.github.io/publication/2025-02-01-aaai-phan1},
    abstract    = {Anytime multi-agent path finding (MAPF) is a promising approach to scalable and collision-free path optimization in multi-agent systems. MAPF-LNS, based on Large Neighborhood Search (LNS), is the current state-of-the-art approach where a fast initial solution is iteratively optimized by destroying and repairing selected paths of the solution. Current MAPF-LNS variants commonly use an adaptive selection mechanism to choose among multiple destroy heuristics. However, to determine promising destroy heuristics, MAPF-LNS requires a considerable amount of exploration time. As common destroy heuristics are stationary, i.e., non-adaptive, any performance bottleneck caused by them cannot be overcome by adaptive heuristic selection alone, thus limiting the overall effectiveness of MAPF-LNS. In this paper, we propose Adaptive Delay-based Destroy-and-Repair Enhanced with Success-based Self-Learning (ADDRESS) as a single-destroy-heuristic variant of MAPF-LNS. ADDRESS applies restricted Thompson Sampling to the top-K set of the most delayed agents to select a seed agent for adaptive LNS neighborhood generation. We evaluate ADDRESS in multiple maps from the MAPF benchmark set and demonstrate cost improvements by at least 50% in large-scale scenarios with up to a thousand agents, compared with the original MAPF-LNS and other state-of-the-art methods.},
    journal     = {Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)},
    year        = {2025},
    volume      = {39},
    number      = {22},
    note        = {*Equal contribution},
    doi         = {https://doi.org/10.1609/aaai.v39i22.34495},
    eprint      = {https://thomyphan.github.io/files/2025-aaai-preprint1.pdf},
    code        = {https://github.com/JimyZ13/ADDRESS},
    month       = {April},
    pages       = {23286--23294}
}

@article{phan2AAAI25,
    author      = {Thomy Phan and Shao-Hung Chan and Sven Koenig},
    title       = {Counterfactual Online Learning for Open-Loop Monte-Carlo Planning},
    url         = {https://thomyphan.github.io/publication/2025-02-01-aaai-phan2},
    abstract    = {Monte-Carlo Tree Search (MCTS) is a popular approach to online planning under uncertainty. While MCTS uses statistical sampling via multi-armed bandits to avoid exhaustive search in complex domains, common closed-loop approaches typically construct enormous search trees to consider a large number of potential observations and actions. On the other hand, open-loop approaches offer better memory efficiency by ignoring observations but are generally not competitive with closed-loop MCTS in terms of performance -- even with commonly integrated human knowledge. In this paper, we propose Counterfactual Open-loop Reasoning with Ad hoc Learning (CORAL) for open-loop MCTS, using a causal multi-armed bandit approach with unobserved confounders (MABUC). CORAL consists of two online learning phases that are conducted during the open-loop search. In the first phase, an intent policy is learned based on preferred actions. In the second phase, a counterfactual policy is learned with MABUCs to make a final decision using the previously learned intent policy. We evaluate CORAL in four POMDP benchmark scenarios and compare it with closed-loop and open-loop alternatives. In contrast to standard open-loop MCTS, CORAL achieves competitive performance compared with closed-loop algorithms while constructing significantly smaller search trees.},
    journal     = {Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)},
    year        = {2025},
    volume      = {39},
    number      = {25},
    doi         = {https://doi.org/10.1609/aaai.v39i25.34867},
    code        = {https://github.com/thomyphan/counterfactual-planning},
    eprint      = {https://thomyphan.github.io/files/2025-aaai-preprint2.pdf},
    month       = {April},
    pages       = {26651--26658}
}

@article{phanAAAI24,
    author      = {Thomy Phan and Taoan Huang and Bistra Dilkina and Sven Koenig},
    title       = {Adaptive Anytime Multi-Agent Path Finding Using Bandit-Based Large Neighborhood Search},
    url         = {https://thomyphan.github.io/publication/2024-02-01-aaai-phan},
    abstract    = {Anytime multi-agent path finding (MAPF) is a promising approach to scalable path optimization in large-scale multi-agent systems. State-of-the-art anytime MAPF is based on Large Neighborhood Search (LNS), where a fast initial solution is iteratively optimized by destroying and repairing a fixed number of parts, i.e., the neighborhood, of the solution, using randomized destroy heuristics and prioritized planning. Despite their recent success in various MAPF instances, current LNS-based approaches lack exploration and flexibility due to greedy optimization with a fixed neighborhood size which can lead to low quality solutions in general. So far, these limitations have been addressed with extensive prior effort in tuning or offline machine learning beyond actual planning. In this paper, we focus on online learning in LNS and propose Bandit-based Adaptive LArge Neighborhood search Combined with Exploration (BALANCE). BALANCE uses a bi-level multi-armed bandit scheme to adapt the selection of destroy heuristics and neighborhood sizes on the fly during search. We evaluate BALANCE on multiple maps from the MAPF benchmark set and empirically demonstrate cost improvements of at least 50% compared to state-of-the-art anytime MAPF in large-scale scenarios. We find that Thompson Sampling performs particularly well compared to alternative multi-armed bandit algorithms.},
    journal     = {Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)},
    year        = {2024},
    volume      = {38},
    number      = {16},
    doi         = {https://doi.org/10.1609/aaai.v38i16.29701},
    eprint      = {https://thomyphan.github.io/files/2024-aaai-preprint.pdf},
    code        = {https://github.com/thomyphan/anytime-mapf},
    month       = {March},
    pages       = {17514--17522}
}

@article{phanJAAMAS2024,
    author      = {Thomy Phan and Felix Sommer and Fabian Ritz and Philipp Altmann and Jonas Nüßlein and Michael Kölle and Lenz Belzner and Claudia Linnhoff-Popien},
    title       = {Emergent Cooperation from Mutual Acknowledgment Exchange in Multi-Agent Reinforcement Learning},
    year        = {2024},
    publisher   = {Springer Nature},
    volume      = {38},
    number      = {34},
    doi         = {https://doi.org/10.1007/s10458-024-09666-5},
    url         = {https://thomyphan.github.io/publication/2024-07-01-jaamas-phan},
    eprint      = {https://link.springer.com/content/pdf/10.1007/s10458-024-09666-5.pdf},
    code        = {https://github.com/thomyphan/emergent-cooperation},
    abstract    = {Peer incentivization (PI) is a recent approach where all agents learn to reward or penalize each other in a distributed fashion, which often leads to emergent cooperation. Current PI mechanisms implicitly assume a flawless communication channel in order to exchange rewards. These rewards are directly incorporated into the learning process without any chance to respond with feedback. Furthermore, most PI approaches rely on global information, which limits scalability and applicability to real-world scenarios where only local information is accessible. In this paper, we propose Mutual Acknowledgment Token Exchange (MATE), a PI approach defined by a two-phase communication protocol to exchange acknowledgment tokens as incentives to shape individual rewards mutually. All agents condition their token transmissions on the locally estimated quality of their own situations based on environmental rewards and received tokens. MATE is completely decentralized and only requires local communication and information. We evaluate MATE in three social dilemma domains. Our results show that MATE is able to achieve and maintain significantly higher levels of cooperation than previous PI approaches. In addition, we evaluate the robustness of MATE in more realistic scenarios, where agents can deviate from the protocol and communication failures can occur.  We also evaluate the sensitivity of MATE w.r.t. the choice of token values.},
    journal     = {Autonomous Agents and Multi-Agent Systems},
    month       = {July},
    keywords    = {Multi-Agent Learning, Reinforcement Learning, Mutual Acknowledgments, Peer Incentivization, Emergent Cooperation},
    note        = {Invited from AAMAS 2022}
}

@inproceedings{phanAAMAS24,
    author      = {Thomy Phan and Joseph Driscoll and Justin Romberg and Sven Koenig},
    title       = {Confidence-Based Curriculum Learning for Multi-Agent Path Finding},
    year        = {2024},
    publisher   = {International Foundation for Autonomous Agents and Multiagent Systems},
    abstract    = {A wide range of real-world applications can be formulated as Multi-Agent Path Finding (MAPF) problem, where the goal is to find collision-free paths for multiple agents with individual start and goal locations. State-of-the-art MAPF solvers are mainly centralized and depend on global information, which limits their scalability and flexibility regarding changes or new maps that would require expensive replanning. Multi-agent reinforcement learning (MARL) offers an alternative way by learning decentralized policies that can generalize over a variety of maps. While there exist some prior works that attempt to connect both areas, the proposed techniques are heavily engineered and very complex due to the integration of many mechanisms that limit generality and are expensive to use. We argue that much simpler and general approaches are needed to bring the areas of MARL and MAPF closer together with significantly lower costs. In this paper, we propose Confidence-based Auto-Curriculum for Team Update Stability (CACTUS) as a lightweight MARL approach to MAPF. CACTUS defines a simple reverse curriculum scheme, where the goal of each agent is randomly placed within an allocation radius around the agent's start location. The allocation radius increases gradually as all agents improve, which is assessed by a confidence-based measure. We evaluate CACTUS in various maps of different sizes, obstacle densities, and numbers of agents. Our experiments demonstrate better performance and generalization capabilities than state-of-the-art approaches while using less than 600,000 trainable parameters, which is less than 5% of the neural network size of current MARL approaches to MAPF.},
    booktitle   = {Proceedings of the 23rd International Conference on Autonomous Agents and MultiAgent Systems (AAMAS)},
    keywords    = {multi-agent path finding, multi-agent reinforcement learning, curriculum learning},
    location    = {Auckland, New Zealand},
    doi         = {https://dl.acm.org/doi/10.5555/3635637.3663016},
    url         = {https://thomyphan.github.io/publication/2024-05-01-aamas-phan},
    eprint      = {https://thomyphan.github.io/files/2024-aamas.pdf},
    code        = {https://github.com/thomyphan/rl4mapf},
    pages       = {1558--1566}
}

@inproceedings{chanAAMAS24,
    author      = {Shao-Hung Chan and Zhe Chen and Dian-Lun Lin and Yue Zhang and Daniel Harabor and Sven Koenig and Tsung-Wei Huang and Thomy Phan},
    title       = {Anytime Multi-Agent Path Finding using Operator Parallelism in Large Neighborhood Search},
    year        = {2024},
    publisher   = {International Foundation for Autonomous Agents and Multiagent Systems},
    abstract    = {Multi-Agent Path Finding (MAPF) is the problem of finding a set of collision-free paths for multiple agents in a shared environment while minimizing the sum of travel times. Since the MAPF problem is NP-hard to solve optimally, anytime algorithms are promising to quickly find a solution and keep optimizing it before interrupting. The current state-of-the-art anytime algorithm for MAPF is based on Large Neighborhood Search (LNS), called MAPF-LNS, which is a combinatorial search algorithm that iteratively destroys and repairs a subset of collision-free paths in order to optimize the sum of travel times. However, the destroy and repair operations in MAPF-LNS can be time-consuming, thus limiting the effectiveness due to fewer iterations and scalability w.r.t. the number of agents. In this paper, we propose Destroy-Repair Operation Parallelism for LNS (DROP-LNS), a parallel framework that performs multiple destroy and repair processes simultaneously to explore a larger searching space under a limited time budget. Unlike MAPF-LNS, DROP-LNS is able to exploit parallelized hardware to improve the solution quality. We extend DROP-LNS to two alternatives and conduct experimental evaluations to compare the performance. The results show that DROP-LNS significantly outperforms the state-of-the-art.},
    booktitle   = {Extended Abstracts of the 23rd International Conference on Autonomous Agents and MultiAgent Systems (AAMAS)},
    keywords    = {multi-agent path finding, anytime algorithm, destroy-repair operator parallelism},
    location    = {Auckland, New Zealand},
    doi         = {https://dl.acm.org/doi/10.5555/3635637.3663101},
    url         = {https://thomyphan.github.io/publication/2024-05-01-aamas-chan},
    eprint      = {https://arxiv.org/pdf/2402.01961.pdf},
    pages       = {2183--2185}
}

@inproceedings{koelleICAART24,
    author      = {Michael Kölle and Felix Topp and Thomy Phan and Philipp Altmann and Jonas Nüßlein and Claudia Linnhoff-Popien},
    title       = {Multi-Agent Quantum Reinforcement Learning using Evolutionary Optimization},
    booktitle   = {Proceedings of the 16th International Conference on Agents and Artificial Intelligence (ICAART)},
    year        = {2024},
    pages       = {71--82},
    publisher   = {SciTePress},
    doi         = {https://doi.org/10.5220/0012382800003636},
    isbn        = {978-989-758-680-4},
    url         = {https://thomyphan.github.io/publication/2024-02-01-icaart-koelle},
    eprint      = {https://thomyphan.github.io/files/2024-icaart-preprint.pdf},
    code        = {https://github.com/michaelkoelle/qmarl-evo},
    abstract    = {Multi-Agent Reinforcement Learning is becoming increasingly more important in times of autonomous driving and other smart industrial applications. Simultaneously a promising new approach to Reinforcement Learning arises using the inherent properties of quantum mechanics, reducing the trainable parameters of a model significantly. However, gradient-based Multi-Agent Quantum Reinforcement Learning methods often have to struggle with barren plateaus, holding them back from matching the performance of classical approaches. We build upon an existing approach for gradient free Quantum Reinforcement Learning and propose three genetic variations with Variational Quantum Circuits for Multi-Agent Reinforcement Learning using evolutionary optimization. We evaluate our genetic variations in the Coin Game environment and also compare them to classical approaches. We showed that our Variational Quantum Circuit approaches perform significantly better compared to a neural network with a similar amount of trainable parameters. Compared to the larger neural network, our approaches archive similar results using 97.88\% less parameters.}
}

@phdthesis{phanPhD23,
  type      = {dissertation},
  title     = {Emergence and Resilience in Multi-Agent Reinforcement Learning},
  author    = {Thomy Phan},
  school    = {LMU Munich},
  url       = {https://thomyphan.github.io/publication/2023-06-26-phd-thesis-phan},
  eprint    = {https://thomyphan.github.io/files/PhD-Thesis-ThomyPhan.pdf},
  year      = {2023},
  doi       = {https://doi.org/10.5282/edoc.31981}
}

@inproceedings{phanICML23,
    author      = {Thomy Phan and Fabian Ritz and Philipp Altmann and Maximilian Zorn and Jonas Nüßlein and Michael Kölle and Thomas Gabor and Claudia Linnhoff-Popien},
    title       = {Attention-Based Recurrence for Multi-Agent Reinforcement Learning under Stochastic Partial Observability},
    year        = {2023},
    abstract    = {Stochastic partial observability poses a major challenge for decentralized coordination in multi-agent reinforcement learning but is largely neglected in state-of-the-art research due to a strong focus on state-based centralized training for decentralized execution (CTDE) and benchmarks that lack sufficient stochasticity like StarCraft Multi-Agent Challenge (SMAC). In this paper, we propose Attention-based Embeddings of Recurrence In multi-Agent Learning (AERIAL) to approximate value functions under stochastic partial observability. AERIAL replaces the true state with a learned representation of multi-agent recurrence, considering more accurate information about decentralized agent decisions than state-based CTDE. We then introduce MessySMAC, a modified version of SMAC with stochastic observations and higher variance in initial states, to provide a more general and configurable benchmark regarding stochastic partial observability. We evaluate AERIAL in Dec-Tiger as well as in a variety of SMAC and MessySMAC maps, and compare the results with state-based CTDE. Furthermore, we evaluate the robustness of AERIAL and state-based CTDE against various stochasticity configurations in MessySMAC.},
    publisher   = {PMLR},
    booktitle   = {Proceedings of the 40th International Conference on Machine Learning (ICML)},
    keywords    = {Dec-POMDP, stochastic partial observability, multi-agent learning, recurrence, self-attention},
    location    = {Hawaii, USA},
    pages       = {27840--27853},
    url         = {https://thomyphan.github.io/publication/2023-07-01-icml-phan},
    code        = {https://github.com/thomyphan/messy_smac},
    eprint      = {https://thomyphan.github.io/files/2023-icml-preprint.pdf},
    note        = {A short version appeared at AAMAS 2023}
}

@inproceedings{altmannIJCAI23,
    author      = {Philipp Altmann and Leonard Feuchtinger and Fabian Ritz and Jonas Nüßlein and Claudia Linnhof-Popien and Thomy Phan},
    title       = {CROP: Towards Distributional-Shift Robust Reinforcement Learning Using Compact Reshaped Observation Processing},
    year        = {2023},
    abstract    = {The safe application of reinforcement learning (RL) requires generalization from limited training data to unseen scenarios. Yet, fulfilling tasks under changing circumstances is a key challenge in RL. Current state-of-the-art approaches for generalization apply data augmentation techniques to increase the diversity of training data. Even though this prevents overfitting to the training environment(s), it hinders policy optimization. Crafting a suitable observation, only containing crucial information, has been shown to be a challenging task itself. To improve data efficiency and generalization capabilities, we propose Compact Reshaped Observation Processing (CROP) to reduce the state information used for policy optimization. By providing only relevant information, overfitting to a specific training layout is precluded and generalization to unseen environments is improved. We formulate three CROPs that can be applied to fully observable observation- and action-spaces and provide methodical foundation. We empirically show the improvements of CROP in a distributionally shifted safety gridworld. We furthermore provide benchmark comparisons to full observability and data-augmentation in two different-sized procedurally generated mazes.},
    publisher   = {International Joint Conferences on Artificial Intelligence Organization},
    booktitle   = {Proceedings of the 32nd International Joint Conference on Artificial Intelligence (IJCAI)},
    keywords    = {deep learning, reinforcement learning, robustness},
    location    = {Macao, China},
    pages       = {3414--3422},
    url         = {https://thomyphan.github.io/publication/2023-08-01-ijcai-altmann},
    code        = {https://github.com/philippaltmann/CROP},
    eprint      = {https://thomyphan.github.io/files/2023-ijcai-preprint.pdf}
}

@inproceedings{zornALIFE23,
    author      = {Maximilian Zorn and Steffen Illium and Thomy Phan and Tanja Katharina Kaiser and Claudia Linnhoff-Popien and Thomas Gabor},
    title       = {Social Neural Network Soups with Surprise Minimization},
    booktitle   = {ALIFE 2023: Ghost in the Machine: Proceedings of the 2023 Artificial Life Conference (ALIFE)},
    series      = {ALIFE 2023: Ghost in the Machine: Proceedings of the 2023 Artificial Life Conference (ALIFE)},
    year        = {2023},
    month       = {07},
    pages       = {65--73},
    publisher   = {MIT Press Direct},
    doi         = {https://doi.org/10.1162/isal_a_00671},
    abstract    = {A recent branch of research in artificial life has constructed artificial chemistry systems whose particles are dynamic neural networks. These particles can be applied to each other and show a tendency towards self-replication of their weight values. We define new interactions for said particles that allow them to recognize one another and learn predictors for each other's behavior. For instance, each particle minimizes its surprise when observing another particle's behavior. Given a special catalyst particle to exert evolutionary selection pressure on the soup of particles, these 'social' interactions are sufficient to produce emergent behavior similar to the stability-pattern previously only achieved via explicit self-replication training.},
    url         = {https://thomyphan.github.io/publication/2023-07-01-alife-zorn},
    eprint      = {https://thomyphan.github.io/files/2023-alife-preprint.pdf}
}

@inproceedings{phanAAMAS22,
    author      = {Thomy Phan and Felix Sommer and Philipp Altmann and Fabian Ritz and Lenz Belzner and Claudia Linnhoff-Popien},
    title       = {Emergent Cooperation from Mutual Acknowledgment Exchange},
    year        = {2022},
    isbn        = {9781450392136},
    publisher   = {International Foundation for Autonomous Agents and Multiagent Systems},
    abstract    = {Peer incentivization (PI) is a recent approach, where all agents learn to reward or to penalize each other in a distributed fashion which often leads to emergent cooperation. Current PI mechanisms implicitly assume a flawless communication channel in order to exchange rewards. These rewards are directly integrated into the learning process without any chance to respond with feedback. Furthermore, most PI approaches rely on global information which limits scalability and applicability to real-world scenarios, where only local information is accessible. In this paper, we propose Mutual Acknowledgment Token Exchange (MATE), a PI approach defined by a two-phase communication protocol to mutually exchange acknowledgment tokens to shape individual rewards. Each agent evaluates the monotonic improvement of its individual situation in order to accept or reject acknowledgment requests from other agents. MATE is completely decentralized and only requires local communication and information. We evaluate MATE in three social dilemma domains. Our results show that MATE is able to achieve and maintain significantly higher levels of cooperation than previous PI approaches. In addition, we evaluate the robustness of MATE in more realistic scenarios, where agents can defect from the protocol and where communication failures can occur.},
    booktitle   = {Proceedings of the 21st International Conference on Autonomous Agents and MultiAgent Systems (AAMAS)},
    pages       = {1047--1055},
    numpages    = {9},
    keywords    = {mutual acknowledgments, emergent cooperation, peer incentivization, reinforcement learning, multi-agent learning},
    location    = {Virtual Event, New Zealand},
    doi         = {https://dl.acm.org/doi/abs/10.5555/3535850.3535967},
    url         = {https://thomyphan.github.io/publication/2022-05-01-aamas-phan},
    code        = {https://github.com/thomyphan/emergent-cooperation},
    eprint      = {https://thomyphan.github.io/files/2022-aamas.pdf}
}

@inproceedings{muellerAAMAS22,
    author      = {Robert Müller and Steffen Illium and Thomy Phan and Tom Haider and Claudia Linnhoff-Popien},
    title       = {Towards Anomaly Detection in Reinforcement Learning},
    year        = {2022},
    isbn        = {9781450392136},
    publisher   = {International Foundation for Autonomous Agents and Multiagent Systems},
    abstract    = {Identifying datapoints that substantially differ from normality is the task of anomaly detection (AD). While AD has gained widespread attention in rich data domains such as images, videos, audio and text, it has has been studied less frequently in the context of reinforcement learning (RL). This is due to the additional layer of complexity that RL introduces through sequential decision making. Developing suitable anomaly detectors for RL is of particular importance in safety-critical scenarios where acting on anomalous data could result in hazardous situations. In this work, we address the question of what AD means in the context of RL. We found that current research trains and evaluates on overly simplistic and unrealistic scenarios which reduce to classic pattern recognition tasks. We link AD in RL to various fields in RL such as lifelong RL and generalization. We discuss their similarities, differences, and how the fields can benefit from each other. Moreover, we identify non-stationarity to be one of the key drivers for future research on AD in RL and make a first step towards a more formal treatment of the problem by framing it in terms of the recently introduced block contextual Markov decision process. Finally, we define a list of practical desiderata for future problems.},
    booktitle   = {BlueSky Ideas of the 21st International Conference on Autonomous Agents and MultiAgent Systems (AAMAS)},
    pages       = {1799--1803},
    numpages    = {5},
    keywords    = {anomaly detection, AI safety, reinforcement learning},
    location    = {Virtual Event, New Zealand},
    doi         = {https://dl.acm.org/doi/10.5555/3535850.3536113},
    url         = {https://thomyphan.github.io/publication/2022-05-01-aamas-mueller},
    eprint      = {https://thomyphan.github.io/files/2022-aamas-bluesky.pdf}
}

@inproceedings{ritzICAARTBook22,
    author      = {Fabian Ritz and Thomy Phan and Robert Müller and Thomas Gabor and Andreas Sedlmeier and Marc Zeller and Jan Wieghardt and Reiner Schmid and Horst Sauer and Cornel Klein and Claudia Linnhoff-Popien},
    title       = {Specification Aware Multi-Agent Reinforcement Learning},
    year        = {2022},
    isbn        = {978-3-031-10160-1},
    publisher   = {Springer-Verlag},
    doi         = {https://doi.org/10.1007/978-3-031-10161-8_1},
    url         = {https://thomyphan.github.io/publication/2022-01-01-icaart-ritz},
    eprint      = {https://arxiv.org/pdf/2012.07949.pdf},
    abstract    = {Engineering intelligent industrial systems is challenging due to high complexity and uncertainty with respect to domain dynamics and multiple agents. If industrial systems act autonomously, their choices and results must be within specified bounds to satisfy these requirements. Reinforcement learning (RL) is promising to find solutions that outperform known or handcrafted heuristics. However in industrial scenarios, it also is crucial to prevent RL from inducing potentially undesired or even dangerous behavior. This paper considers specification alignment in industrial scenarios with multi-agent reinforcement learning (MARL). We propose to embed functional and non-functional requirements into the reward function, enabling the agents to learn to align with the specification. We evaluate our approach in a smart factory simulation representing an industrial lot-size-one production facility, where we train up to eight agents using DQN, VDN, and QMIX. Our results show that the proposed approach enables agents to satisfy a given set of requirements.},
    booktitle   = {Agents and Artificial Intelligence: 13th International Conference (ICAART 2021), Revised Selected Papers},
    pages       = {3--21},
    numpages    = {19},
    keywords    = {Reinforcement learning, Multi-agent, Specification compliance, AI safety}
}

@inproceedings{ritzISoLA22,
    author      = {Fabian Ritz and Thomy Phan and Andreas Sedlmeier and Philipp Altmann and Jan Wieghardt and Reiner Schmid and Horst Sauer and Cornel Klein and Claudia Linnhoff-Popien and Thomas Gabor},
    editor      = {Margaria, Tiziana and Steffen, Bernhard},
    title       = {Capturing Dependencies Within Machine Learning via a Formal Process Model},
    booktitle   = {International Symposium on Leveraging Applications of Formal Methods, Verification and Validation (ISoLA)},
    year        = {2022},
    publisher   = {Springer Nature Switzerland},
    pages       = {249--265},
    isbn        = {978-3-031-19759-8},
    doi         = {https://doi.org/10.1007/978-3-031-19759-8_16},
    url         = {https://thomyphan.github.io/publication/2022-10-01-isola-ritz},
    eprint      = {https://thomyphan.github.io/files/2022-isola-preprint.pdf},
    abstract    = {The development of Machine Learning (ML) models is more than just a special case of software development (SD): ML models acquire properties and fulfill requirements even without direct human interaction in a seemingly uncontrollable manner. Nonetheless, the underlying processes can be described in a formal way. We define a comprehensive SD process model for ML that encompasses most tasks and artifacts described in the literature in a consistent way. In addition to the production of the necessary artifacts, we also focus on generating and validating fitting descriptions in the form of specifications. We stress the importance of further evolving the ML model throughout its life-cycle even after initial training and testing. Thus, we provide various interaction points with standard SD processes in which ML often is an encapsulated task. Further, our SD process model allows to formulate ML as a (meta-) optimization problem. If automated rigorously, it can be used to realize self-adaptive autonomous systems. Finally, our SD process model features a description of time that allows to reason about the progress within ML development processes. This might lead to further applications of formal methods within the field of ML.}
}

@inproceedings{phanNeurIPS21,
    author      = {Thomy Phan and Fabian Ritz and Lenz Belzner and Philipp Altmann and Thomas Gabor and Claudia Linnhoff-Popien},
    booktitle   = {Advances in Neural Information Processing Systems (NeurIPS)},
    editor      = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
    pages       = {24018--24032},
    abstract    = {Value function factorization (VFF) is a popular approach to cooperative multi-agent reinforcement learning in order to learn local value functions from global rewards. However, state-of-the-art VFF is limited to a handful of agents in most domains. We hypothesize that this is due to the flat factorization scheme, where the VFF operator becomes a performance bottleneck with an increasing number of agents. Therefore, we propose VFF with variable agent sub-teams (VAST). VAST approximates a factorization for sub-teams which can be defined in an arbitrary way and vary over time, e.g., to adapt to different situations. The sub-team values are then linearly decomposed for all sub-team members. Thus, VAST can learn on a more focused and compact input representation of the original VFF operator. We evaluate VAST in three multi-agent domains and show that VAST can significantly outperform state-of-the-art VFF, when the number of agents is sufficiently large.},
    publisher   = {Curran Associates, Inc.},
    title       = {VAST: Value Function Factorization with Variable Agent Sub-Teams},
    url         = {https://thomyphan.github.io/publication/2021-12-01-neurips-phan},
    eprint      = {https://proceedings.neurips.cc/paper/2021/file/c97e7a5153badb6576d8939469f58336-Paper.pdf},
    volume      = {34},
    year        = {2021},
    code        = {https://github.com/thomyphan/scalable-marl}
}

@article{phanAAAI21,
    author      = {Thomy Phan and Lenz Belzner and Thomas Gabor and Andreas Sedlmeier and Fabian Ritz and Claudia Linnhoff-Popien},
    title       = {Resilient Multi-Agent Reinforcement Learning with Adversarial Value Decomposition},
    volume      = {35},
    url         = {https://thomyphan.github.io/publication/2021-02-01-aaai-phan},
    doi         = {https://doi.org/10.1609/aaai.v35i13.17348},
    eprint      = {https://thomyphan.github.io/files/2021-aaai.pdf},
    abstract    = {We focus on resilience in cooperative multi-agent systems, where agents can change their behavior due to udpates or failures of hardware and software components. Current state-of-the-art approaches to cooperative multi-agent reinforcement learning (MARL) have either focused on idealized settings without any changes or on very specialized scenarios, where the number of changing agents is fixed, e.g., in extreme cases with only one productive agent. Therefore, we propose Resilient Adversarial value Decomposition with Antagonist-Ratios (RADAR). RADAR offers a value decomposition scheme to train competing teams of varying size for improved resilience against arbitrary agent changes. We evaluate RADAR in two cooperative multi-agent domains and show that RADAR achieves better worst case performance w.r.t. arbitrary agent changes than state-of-the-art MARL.},
    number      = {13},
    journal     = {Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)},
    year        = {2021},
    month       = {May},
    pages       = {11308--11316},
    code        = {https://github.com/thomyphan/resilient-marl}
}

@article{gaborNACO21,
    author      = {Thomas Gabor and Thomy Phan and Claudia Linnhoff-Popien},
    title       = {Productive Fitness in Diversity-Aware Evolutionary Algorithms},
    year        = {2021},
    publisher   = {Kluwer Academic Publishers},
    volume      = {20},
    number      = {3},
    issn        = {1567-7818},
    doi         = {https://doi.org/10.1007/s11047-021-09853-3},
    url         = {https://thomyphan.github.io/publication/2021-01-01-naco-gabor},
    eprint      = {https://link.springer.com/content/pdf/10.1007/s11047-021-09853-3.pdf},
    abstract    = {In evolutionary algorithms, the notion of diversity has been adopted from biology and is used to describe the distribution of a population of solution candidates. While it has been known that maintaining a reasonable amount of diversity often benefits the overall result of the evolutionary optimization process by adjusting the exploration/exploitation trade-off, little has been known about what diversity is optimal. We introduce the notion of productive fitness based on the effect that a specific solution candidate has some generations down the evolutionary path. We derive the notion of final productive fitness, which is the ideal target fitness for any evolutionary process. Although it is inefficient to compute, we show empirically that it allows for an a posteriori analysis of how well a given evolutionary optimization process hit the ideal exploration/exploitation trade-off, providing insight into why diversity-aware evolutionary optimization often performs better.},
    journal     = {Natural Computing},
    month       = {September},
    pages       = {363--376},
    numpages    = {14},
    keywords    = {Evolutionary algorithm, Diversity, Adaptive fitness}
}

@inproceedings{ritzALIFE21,
    author      = {Fabian Ritz and Daniel Ratke and Thomy Phan and Lenz Belzner and Claudia Linnhoff-Popien},
    title       = {A Sustainable Ecosystem through Emergent Cooperation in Multi-Agent Reinforcement Learning},
    booktitle   = {Conference on Artificial Life (ALIFE)},
    year        = {2021},
    month       = {07},
    pages       = {74--83},
    publisher   = {MIT Press Direct},
    abstract    = {This paper considers sustainable and cooperative behavior in multi-agent systems. In the proposed predator-prey simulation, multiple selfish predators can learn to act sustainably by maintaining a herd of reproducing prey and further hunt cooperatively for long term benefit. Since the predators face starvation pressure, the scenario can also turn in a tragedy of the commons if selfish individuals decide to greedily hunt down the prey population before their conspecifics do, ultimately leading to extinction of prey and predators. This paper uses Multi-Agent Reinforcement Learning to overcome a collapse of the simulated ecosystem, analyzes the impact factors over multiple dimensions and proposes suitable metrics. We show that up to three predators are able to learn sustainable behavior in form of collective herding under starvation pressure. Complex cooperation in form of group hunting emerges between the predators as their speed is handicapped and the prey is given more degrees of freedom to escape. The implementation of environment and reinforcement learning pipeline is available online.},
    url         = {https://thomyphan.github.io/publication/2021-07-01-alife-ritz},
    doi         = {https://doi.org/10.1162/isal_a_00399},
    note        = {74},
    eprint      = {https://direct.mit.edu/isal/proceedings-pdf/isal/33/74/1930024/isal_a_00399.pdf},
    code        = {https://github.com/instance01/fish-rl-alife}
}

@inproceedings{ritzICAART21,
    author      = {Fabian Ritz and Thomy Phan and Robert Müller and Thomas Gabor and Andreas Sedlmeier and Marc Zeller and Jan Wieghardt and Reiner Schmid and Horst Sauer and Cornel Klein and Claudia Linnhoff-Popien},
    title       = {SAT-MARL: Specification Aware Training in Multi-Agent Reinforcement Learning},
    booktitle   = {Proceedings of the 13th International Conference on Agents and Artificial Intelligence (ICAART)},
    year        = {2021},
    pages       = {28--37},
    publisher   = {SciTePress},
    doi         = {https://doi.org/10.5220/0010189500280037},
    isbn        = {978-989-758-484-8},
    url         = {https://thomyphan.github.io/publication/2021-02-01-icaart-ritz},
    eprint      = {https://thomyphan.github.io/files/2021-icaart-preprint.pdf},
    abstract    = {A characteristic of reinforcement learning is the ability to develop unforeseen strategies when solving problems. While such strategies sometimes yield superior performance, they may also result in undesired or even dangerous behavior. In industrial scenarios, a system’s behavior also needs to be predictable and lie within defined ranges. To enable the agents to learn (how) to align with a given specification, this paper proposes to explicitly transfer functional and non-functional requirements into shaped rewards. Experiments are carried out on the smart factory, a multi-agent environment modeling an industrial lot-size-one production facility, with up to eight agents and different multi-agent reinforcement learning algorithms. Results indicate that compliance with functional and non-functional constraints can be achieved by the proposed approach.}
}

@inproceedings{phanAAMAS20,
    author      = {Thomy Phan and Thomas Gabor and Andreas Sedlmeier and Fabian Ritz and Bernhard Kempter and Cornel Klein and Horst Sauer and Reiner Schmid and Jan Wieghardt and Marc Zeller and Claudia Linnhoff-Popien},
    title       = {Learning and Testing Resilience in Cooperative Multi-Agent Systems},
    year        = {2020},
    isbn        = {9781450375184},
    publisher   = {International Foundation for Autonomous Agents and Multiagent Systems},
    abstract    = {State-of-the-art multi-agent reinforcement learning has achieved remarkable success in recent years. The success has been mainly based on the assumption that all teammates perfectly cooperate to optimize a global objective in order to achieve a common goal. While this may be true in the ideal case, these approaches could fail in practice, since in multi-agent systems (MAS), all agents may be a potential source of failure. In this paper, we focus on resilience in cooperative MAS and propose an Antagonist-Ratio Training Scheme (ARTS) by reformulating the original target MAS as a mixed cooperative-competitive game between a group of protagonists which represent agents of the target MAS and a group of antagonists which represent failures in the MAS. While the protagonists can learn robust policies to ensure resilience against failures, the antagonists can learn malicious behavior to provide an adequate test suite for other MAS. We empirically evaluate ARTS in a cyber physical production domain and show the effectiveness of ARTS w.r.t. resilience and testing capabilities.},
    booktitle   = {Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems (AAMAS)},
    pages       = {1055--1063},
    numpages    = {9},
    keywords    = {learning and testing, multi-agent learning, adversarial learning},
    location    = {Virtual Event, New Zealand},
    url         = {https://thomyphan.github.io/publication/2020-05-01-aamas-phan},
    eprint      = {https://thomyphan.github.io/files/2020-aamas.pdf},
    doi         = {https://dl.acm.org/doi/10.5555/3398761.3398884}
}

@article{gaborSTTT20,
    title       = {The Scenario Coevolution Paradigm: Adaptive Quality Assurance for Adaptive Systems},
    author      = {Thomas Gabor and Andreas Sedlmeier and Thomy Phan and Fabian Ritz and Marie Kiermeier and Lenz Belzner and Bernhard Kempter and Cornel Klein and Horst Sauer and Reiner Schmid and Jan Wieghardt and Marc Zeller and Claudia Linnhoff-Popien},
    journal     = {International Journal on Software Tools for Technology Transfer (STTT)},
    volume      = {22},
    number      = {4},
    pages       = {457--476},
    year        = {2020},
    publisher   = {Springer},
    abstract    = {Systems are becoming increasingly more adaptive, using techniques like machine learning to enhance their behavior on their own rather than only through human developers programming them. We analyze the impact the advent of these new techniques has on the discipline of rigorous software engineering, especially on the issue of quality assurance. To this end, we provide a general description of the processes related to machine learning and embed them into a formal framework for the analysis of adaptivity, recognizing that to test an adaptive system a new approach to adaptive testing is necessary. We introduce scenario coevolution as a design pattern describing how system and test can work as antagonists in the process of software evolution. While the general pattern applies to large-scale processes (including human developers further augmenting the system), we show all techniques on a smaller-scale example of an agent navigating a simple smart factory. We point out new aspects in software engineering for adaptive systems that may be tackled naturally using scenario coevolution. This work is a substantially extended take on Gabor et al. (International symposium on leveraging applications of formal methods, Springer, pp 137–154, 2018)},
    doi         = {https://doi.org/10.1007/s10009-020-00560-5},
    url         = {https://thomyphan.github.io/publication/2020-01-01-sttt-gabor},
    eprint      = {https://epub.ub.uni-muenchen.de/73060/1/Gabor2020_Article_TheScenarioCoevolutionParadigm.pdf}
}

@inproceedings{rochICCS20,
    title       = {A Quantum Annealing Algorithm for Finding Pure Nash Equilibria in Graphical Games},
    author      = {Christoph Roch and Thomy Phan and Sebastian Feld and Robert Müller and Thomas Gabor and Carsten Hahn and Claudia Linnhoff-Popien},
    booktitle   = {International Conference on Computational Science (ICCS)},
    pages       = {488--501},
    year        = {2020},
    publisher   = {Springer},
    abstract    = {We introduce Q-Nash, a quantum annealing algorithm for the NP-complete problem of finding pure Nash equilibria in graphical games. The algorithm consists of two phases. The first phase determines all combinations of best response strategies for each player using classical computation. The second phase finds pure Nash equilibria using a quantum annealing device by mapping the computed combinations to a quadratic unconstrained binary optimization formulation based on the Set Cover problem. We empirically evaluate Q-Nash on D-Wave’s Quantum Annealer 2000Q using different graphical game topologies. The results with respect to solution quality and computing time are compared to a Brute Force algorithm and the Iterated Best Response heuristic.},
    doi         = {https://doi.org/10.1007/978-3-030-50433-5_38},
    url         = {https://thomyphan.github.io/publication/2020-08-01-iccs-roch},
    eprint      = {https://arxiv.org/pdf/1903.06454.pdf}
}

@inproceedings{ritzALIFE20,
    author      = {Fabian Ritz and Felix Hohnstein and Robert Müller and Thomy Phan and Thomas Gabor and Carsten Hahn and Claudia Linnhoff-Popien},
    title       = {Towards Ecosystem Management from Greedy Reinforcement Learning in a Predator-Prey Setting},
    booktitle   = {Conference on Artificial Life (ALIFE)},
    pages       = {518--525},
    year        = {2020},
    month       = {07},
    publisher   = {MIT Press Direct},
    abstract    = {This paper applies reinforcement learning to train a predator to hunt multiple prey, which are able to reproduce, in a 2D simulation. It is shown that, using methods of curriculum learning, long-term reward discounting and stacked observations, a reinforcement-learning-based predator can achieve an economic strategy: Only hunt when there is still prey left to reproduce in order to maintain the population. Hence, purely selfish goals are sufficient to motivate a reinforcement learning agent for long-term planning and keeping a certain balance with its environment by not depleting its resources. While a comparably simple reinforcement learning algorithm achieves such behavior in the present scenario, providing a suitable amount of past and predictive information turns out to be crucial for the training success.},
    url         = {https://thomyphan.github.io/publication/2020-07-01-alife-ritz},
    doi         = {https://doi.org/10.1162/isal_a_00273},
    eprint      = {https://direct.mit.edu/isal/proceedings-pdf/isal2020/32/518/1908444/isal_a_00273.pdf}
}

@inproceedings{hahnALIFE20,
    author      = {Carsten Hahn and Fabian Ritz and Paula Wikidal and Thomy Phan and Thomas Gabor and Claudia Linnhoff-Popien},
    title       = {Foraging Swarms Using Multi-Agent Reinforcement Learning},
    booktitle   = {Conference on Artificial Life (ALIFE)},
    pages       = {333--340},
    year        = {2020},
    month       = {07},
    publisher   = {MIT Press Direct},
    abstract    = {Flocking or swarm behavior is a widely observed phenomenon in nature. Although the entities might have self-interested goals like evading predators or foraging, they group themselves together because a collaborative observation is superior to the observation of a single individual. In this paper, we evaluate the emergence of swarms in a foraging task using multi-agent reinforcement learning (MARL). Every individual can move freely in a continuous space with the objective to follow a moving target object in a partially observable environment. The individuals are self-interested as there is no explicit incentive to collaborate with each other. However, our evaluation shows that these individuals learn to form swarms out of self-interest and learn to orient themselves to each other in order to find the target object even when it is out of sight for most individuals.},
    doi         = {https://doi.org/10.1162/isal_a_00267},
    url         = {https://thomyphan.github.io/publication/2020-07-01-alife-hahn},
    eprint      = {https://direct.mit.edu/isal/proceedings-pdf/isal2020/32/333/1908570/isal_a_00267.pdf}
}

@inproceedings{gaborQSE20,
    author      = {Thomas Gabor and Leo Sünkel and Fabian Ritz and Thomy Phan and Lenz Belzner and Christoph Roch and Sebastian Feld and Claudia Linnhoff-Popien},
    title       = {The Holy Grail of Quantum Artificial Intelligence: Major Challenges in Accelerating the Machine Learning Pipeline},
    year        = {2020},
    isbn        = {9781450379632},
    publisher   = {Association for Computing Machinery},
    eprint      = {https://thomyphan.github.io/files/2020-qse-preprint.pdf},
    doi         = {https://doi.org/10.1145/3387940.3391469},
    url         = {https://thomyphan.github.io/publication/2020-08-01-qse-gabor},
    abstract    = {We discuss the synergetic connection between quantum computing and artificial intelligence. After surveying current approaches to quantum artificial intelligence and relating them to a formal model for machine learning processes, we deduce four major challenges for the future of quantum artificial intelligence: (i) Replace iterative training with faster quantum algorithms, (ii) distill the experience of larger amounts of data into the training process, (iii) allow quantum and classical components to be easily combined and exchanged, and (iv) build tools to thoroughly analyze whether observed benefits really stem from quantum properties of the algorithm.},
    booktitle   = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering (ICSE) Workshops},
    pages       = {456--461},
    numpages    = {6},
    keywords    = {artificial intelligence, software engineering, quantum computing},
    location    = {Seoul, Republic of Korea}
}

@inproceedings{phanALA20,
    title        = {A Distributed Policy Iteration Scheme for Cooperative Multi-Agent Policy Approximation},
    author       = {Thomy Phan and Lenz Belzner and Kyrill Schmid and Thomas Gabor and Fabian Ritz and Sebastian Feld and Claudia Linnhoff-Popien},
    abstract     = {We propose Stable Emergent Policy (STEP) approximation, a distributed policy iteration scheme to stably approximate decentralized policies for partially observable and cooperative multi-agent systems. STEP offers a novel training architecture, where function approximation is used to learn from action recommendations of a decentralized planning algorithm. Planning is enabled by exploiting a training simulator, which is assumed to be available during centralized learning, and further enhanced by reintegrating the learned policies. We experimentally evaluate STEP in two challenging and stochastic domains, and compare its performance with state-of-the-art multi-agent reinforcement learning algorithms.},
    booktitle    = {12th Adaptive and Learning Agents Workshop (ALA)},
    month        = {May},
    year         = {2020},
    url          = {https://thomyphan.github.io/publication/2020-05-01-ala-phan},
    eprint       = {https://thomyphan.github.io/files/2020-ala.pdf}
}

@article{phanAAAI2019,
    author      = {Thomy Phan and Lenz Belzner and Marie Kiermeier and Markus Friedrich and Kyrill Schmid and Claudia Linnhoff-Popien},
    title       = {Memory Bounded Open-Loop Planning in Large POMDPs Using Thompson Sampling},
    volume      = {33},
    abstract    = {State-of-the-art approaches to partially observable planning like POMCP are based on stochastic tree search. While these approaches are computationally efficient, they may still construct search trees of considerable size, which could limit the performance due to restricted memory resources. In this paper, we propose Partially Observable Stacked Thompson Sampling (POSTS), a memory bounded approach to openloop planning in large POMDPs, which optimizes a fixed size stack of Thompson Sampling bandits. We empirically evaluate POSTS in four large benchmark problems and compare its performance with different tree-based approaches. We show that POSTS achieves competitive performance compared to tree-based open-loop planning and offers a performance memory tradeoff, making it suitable for partially observable planning with highly restricted computational and memory resources.},
    number      = {01},
    journal     = {Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)},
    year        = {2019},
    month       = {July},
    pages       = {7941--7948},
    url         = {https://thomyphan.github.io/publication/2019-02-01-aaai-phan},
    doi         = {https://doi.org/10.1609/aaai.v33i01.33017941},
    eprint      = {https://thomyphan.github.io/files/2019-aaai.pdf},
    code        = {https://github.com/thomyphan/planning}
}

@inproceedings{phanIJCAI19,
    title     = {Adaptive Thompson Sampling Stacks for Memory Bounded Open-Loop Planning},
    author    = {Thomy Phan and Thomas Gabor and Robert Müller and Christoph Roch and Claudia Linnhoff-Popien},
    booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence (IJCAI)},
    publisher = {International Joint Conferences on Artificial Intelligence Organization},
    abstract  = {We propose Stable Yet Memory Bounded Open-Loop (SYMBOL) planning, a general memory bounded approach to partially observable open-loop planning. SYMBOL maintains an adaptive stack of Thompson Sampling bandits, whose size is bounded by the planning horizon and can be automatically adapted according to the underlying domain without any prior domain knowledge beyond a generative model. We empirically test SYMBOL in four large POMDP benchmark problems to demonstrate its effectiveness and robustness w.r.t. the choice of hyperparameters and evaluate its adaptive memory consumption. We also compare its performance with other open-loop planning algorithms and POMCP.},
    pages     = {5607--5613},
    year      = {2019},
    month     = {7},
    url       = {https://thomyphan.github.io/publication/2019-08-01-ijcai-phan},
    eprint    = {https://thomyphan.github.io/files/2019-ijcai-1.pdf},
    doi       = {https://doi.org/10.24963/ijcai.2019/778},
    code      = {https://github.com/thomyphan/planning}
}

@inproceedings{gaborIJCAI19,
    title     = {Subgoal-Based Temporal Abstraction in Monte-Carlo Tree Search},
    author    = {Thomas Gabor and Jan Peter and Thomy Phan and Christian Meyer and Claudia Linnhoff-Popien},
    booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence (IJCAI)},
    publisher = {International Joint Conferences on Artificial Intelligence Organization},
    pages     = {5562--5568},
    abstract  = {We propose an approach to general subgoal-based temporal abstraction in MCTS. Our approach approximates a set of available macro-actions locally for each state only requiring a generative model and a subgoal predicate. For that, we modify the expansion step of MCTS to automatically discover and optimize macro-actions that lead to subgoals. We empirically evaluate the effectiveness, computational efficiency and robustness of our approach w.r.t. different parameter settings in two benchmark domains and compare the results to standard MCTS without temporal abstraction.},
    year      = {2019},
    month     = {7},
    doi       = {https://doi.org/10.24963/ijcai.2019/772},
    url       = {https://thomyphan.github.io/publication/2019-08-01-ijcai-gabor},
    eprint    = {https://thomyphan.github.io/files/2019-ijcai-2.pdf},
    code      = {https://github.com/jnptr/subgoal-mcts}
}

@inproceedings{phanAAMAS19,
    author      = {Thomy Phan and Kyrill Schmid and Lenz Belzner and Thomas Gabor and Sebastian Feld and Claudia Linnhoff-Popien},
    title       = {Distributed Policy Iteration for Scalable Approximation of Cooperative Multi-Agent Policies},
    year        = {2019},
    isbn        = {9781450363099},
    publisher   = {International Foundation for Autonomous Agents and Multiagent Systems},
    abstract    = {We propose Strong Emergent Policy (STEP) approximation, a scalable approach to learn strong decentralized policies for cooperative MAS with a distributed variant of policy iteration. For that, we use function approximation to learn from action recommendations of a decentralized multi-agent planning algorithm. STEP combines decentralized multi-agent planning with centralized learning, only requiring a generative model for distributed black box optimization. We experimentally evaluate STEP in two challenging and stochastic domains with large state and joint action spaces and show that STEP is able to learn stronger policies than standard multi-agent reinforcement learning algorithms, when combining multi-agent open-loop planning with centralized function approximation. The learned policies can be reintegrated into the multi-agent planning process to further improve performance.},
    booktitle   = {Extended Abstracts of the 18th International Conference on Autonomous Agents and MultiAgent Systems (AAMAS)},
    pages       = {2162--2164},
    numpages    = {3},
    keywords    = {multi-agent learning, policy iteration, multi-agent planning},
    location    = {Montreal QC, Canada},
    url         = {https://thomyphan.github.io/publication/2019-05-01-aamas-phan},
    eprint      = {https://arxiv.org/pdf/1901.08761.pdf},
    doi         = {https://dl.acm.org/doi/10.5555/3306127.3332044}
}

@inproceedings{gaborGECCO19,
    author      = {Thomas Gabor and Andreas Sedlmeier and Marie Kiermeier and Thomy Phan and Marcel Henrich and Monika Pichlmair and Bernhard Kempter and Cornel Klein and Horst Sauer and Reiner Schmid and Jan Wieghardt},
    title       = {Scenario Co-Evolution for Reinforcement Learning on a Grid World Smart Factory Domain},
    year        = {2019},
    isbn        = {9781450361118},
    publisher   = {Association for Computing Machinery},
    abstract    = {Adversarial learning has been established as a successful paradigm in reinforcement learning. We propose a hybrid adversarial learner where a reinforcement learning agent tries to solve a problem while an evolutionary algorithm tries to find problem instances that are hard to solve for the current expertise of the agent, causing the intelligent agent to co-evolve with a set of test instances or scenarios. We apply this setup, called scenario co-evolution, to a simulated smart factory problem that combines task scheduling with navigation of a grid world. We show that the so trained agent outperforms conventional reinforcement learning. We also show that the scenarios evolved this way can provide useful test cases for the evaluation of any (however trained) agent.},
    booktitle   = {Proceedings of the Genetic and Evolutionary Computation Conference (GECCO)},
    pages       = {898--906},
    numpages    = {9},
    keywords    = {coevolution, evolutionary algorithms, adversarial learning, reinforcement learning, automatic test generation},
    location    = {Prague, Czech Republic},
    url         = {https://thomyphan.github.io/publication/2019-06-01-gecco-gabor},
    eprint      = {https://thomyphan.github.io/files/2019-gecco.pdf},
    doi         = {https://doi.org/10.1145/3321707.3321831}
}

@inproceedings{hahnALIFE19,
    author      = {Carsten Hahn and Thomy Phan and Thomas Gabor and Lenz Belzner and Claudia Linnhoff-Popien},
    title       = {Emergent Escape-based Flocking Behavior Using Multi-Agent Reinforcement Learning},
    booktitle   = {Conference on Artificial Life (ALIFE)},
    pages       = {598--605},
    year        = {2019},
    month       = {07},
    publisher   = {MIT Press Direct},
    abstract    = {In nature, flocking or swarm behavior is observed in many species as it has beneficial properties like reducing the probability of being caught by a predator. In this paper, we propose SELFish (Swarm Emergent Learning Fish), an approach with multiple autonomous agents which can freely move in a continuous space with the objective to avoid being caught by a present predator. The predator has the property that it might get distracted by multiple possible preys in its vicinity. We show that this property in interaction with self-interested agents which are trained with reinforcement learning solely to survive as long as possible leads to flocking behavior similar to Boids, a common simulation for flocking behavior. Furthermore we present interesting insights into the swarming behavior and into the process of agents being caught in our modeled environment.},
    doi         = {https://doi.org/10.1162/isal_a_00226},
    url         = {https://thomyphan.github.io/publication/2019-07-01-alife-hahn},
    eprint      = {https://direct.mit.edu/isal/proceedings-pdf/isal2019/31/598/1903589/isal_a_00226.pdf}
}

@inproceedings{phanAAMAS18,
    author      = {Thomy Phan and Lenz Belzner and Thomas Gabor and Kyrill Schmid},
    title       = {Leveraging Statistical Multi-Agent Online Planning with Emergent Value Function Approximation},
    year        = {2018},
    publisher   = {International Foundation for Autonomous Agents and Multiagent Systems},
    abstract    = {Making decisions is a great challenge in distributed autonomous environments due to enormous state spaces and uncertainty. Many online planning algorithms rely on statistical sampling to avoid searching the whole state space, while still being able to make acceptable decisions. However, planning often has to be performed under strict computational constraints making online planning in multi-agent systems highly limited, which could lead to poor system performance, especially in stochastic domains. In this paper, we propose Emergent Value function Approximation for Distributed Environments (EVADE), an approach to integrate global experience into multi-agent online planning in stochastic domains to consider global effects during local planning. For this purpose, a value function is approximated online based on the emergent system behaviour by using methods of reinforcement learning. We empirically evaluated EVADE with two statistical multi-agent online planning algorithms in a highly complex and stochastic smart factory environment, where multiple agents need to process various items at a shared set of machines. Our experiments show that EVADE can effectively improve the performance of multi-agent online planning while offering efficiency w.r.t. the breadth and depth of the planning process.},
    booktitle   = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems (AAMAS)},
    pages       = {730--738},
    numpages    = {9},
    keywords    = {online planning, value function approximation, multi-agent planning},
    location    = {Stockholm, Sweden},
    doi         = {https://dl.acm.org/doi/abs/10.5555/3237383.3237491},
    url         = {https://thomyphan.github.io/publication/2018-06-01-aamas-phan},
    eprint      = {https://thomyphan.github.io/files/2018-aamas.pdf}
}

@inproceedings{gaborICAC18,
    author      = {Thomas Gabor and Lenz Belzner and Thomy Phan and Kyrill Schmid},
    title       = {Preparing for the Unexpected: Diversity Improves Planning Resilience in Evolutionary Algorithms}, 
    booktitle   = {2018 IEEE International Conference on Autonomic Computing (ICAC)}, 
    year        = {2018},
    pages       = {131--140},
    abstract    = {As automatic optimization techniques find their way into industrial applications, the behavior of many complex systems is determined by some form of planner picking the right actions to optimize a given objective function. In many cases, the mapping of plans to objective reward may change due to unforeseen events or circumstances in the real world. In those cases, the planner usually needs some additional effort to adjust to the changed situation and reach its previous level of performance. Whenever we still need to continue polling the planner even during re-planning, it oftentimes exhibits severely lacking performance. In order to improve the planner's resilience to unforeseen change, we argue that maintaining a certain level of diversity amongst the considered plans at all times should be added to the planner's objective. Effectively, we encourage the planner to keep alternative plans to its currently best solution. As an example case, we implement a diversity-aware genetic algorithm using two different metrics for diversity (differing in their generality) and show that the blow in performance due to unexpected change can be severely lessened in the average case. We also analyze the parameter settings necessary for these techniques in order to gain an intuition how they can be incorporated into larger frameworks or process models for software and systems engineering.},
    doi         = {https://doi.org/10.1109/ICAC.2018.00023},
    url         = {https://thomyphan.github.io/publication/2018-09-01-icac-gabor},
    eprint      = {https://thomyphan.github.io/files/2018-icac-preprint.pdf}
}

@inproceedings{schmidICANN18,
    author      = {Kyrill Schmid and Lenz Belzner and Thomas Gabor and Thomy Phan},
    editor      = {K{\r{u}}rkov{\'a}, V{\v{e}}ra and Manolopoulos, Yannis and Hammer, Barbara and Iliadis, Lazaros and Maglogiannis, Ilias},
    title       = {Action Markets in Deep Multi-Agent Reinforcement Learning},
    booktitle   = {Artificial Neural Networks and Machine Learning (ICANN)},
    year        = {2018},
    publisher   = {Springer International Publishing},
    pages       = {240--249},
    abstract    = {Recent work on learning in multi-agent systems (MAS) is concerned with the ability of self-interested agents to learn cooperative behavior. In many settings such as resource allocation tasks the lack of cooperative behavior can be seen as a consequence of wrong incentives. I.e., when agents can not freely exchange their resources then greediness is not uncooperative but only a consequence of reward maximization. In this work, we show how the introduction of markets helps to reduce the negative effects of individual reward maximization. To study the emergence of trading behavior in MAS we use Deep Reinforcement Learning (RL) where agents are self-interested, independent learners represented through Deep Q-Networks (DQNs). Specifically, we propose Action Traders, referring to agents that can trade their atomic actions in exchange for environmental reward. For empirical evaluation we implemented action trading in the Coin Game -- and find that trading significantly increases social efficiency in terms of overall reward compared to agents without action trading.},
    isbn        = {978-3-030-01421-6},
    doi         = {https://doi.org/10.1007/978-3-030-01421-6_24},
    url         = {https://thomyphan.github.io/publication/2018-08-01-icann-schmid},
    eprint      = {https://thomyphan.github.io/files/2018-icann.pdf}
}
